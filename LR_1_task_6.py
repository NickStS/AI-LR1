# -*- coding: utf-8 -*-
"""LR1_clean.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15dq6ACSJNVXYHw6BRoT6TxsG6NUM0okC
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn import preprocessing, linear_model
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (confusion_matrix, accuracy_score, recall_score,
                             precision_score, f1_score, roc_curve, roc_auc_score)

def visualize_classifier(classifier, X, y, title=''):
    if X.shape[1] != 2:
        raise ValueError('X must have exactly 2 features')
    x_min, x_max = X[:,0].min()-1.0, X[:,0].max()+1.0
    y_min, y_max = X[:,1].min()-1.0, X[:,1].max()+1.0
    h = 0.02
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)
    plt.figure()
    plt.contourf(xx, yy, Z, alpha=0.25)
    for cls in np.unique(y):
        pts = X[y==cls]
        plt.scatter(pts[:,0], pts[:,1], edgecolor='k')
    if title: plt.title(title)
    plt.tight_layout(); plt.show()

input_data = np.array([[5.1, -2.9, 3.3],
                       [-1.2, 7.8, -6.1],
                       [3.9, 0.4, 2.1],
                       [7.3, -9.9, -4.5]], dtype=float)

data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)
print(data_binarized)

print(input_data.mean(axis=0))
print(input_data.std(axis=0))
data_scaled = preprocessing.scale(input_data)
print(data_scaled.mean(axis=0))
print(data_scaled.std(axis=0))

mm = preprocessing.MinMaxScaler((0,1)).fit_transform(input_data)
print(mm)

l1 = preprocessing.normalize(input_data, norm='l1')
l2 = preprocessing.normalize(input_data, norm='l2')
print(l1); print(l2)

from sklearn import preprocessing as prep
input_labels = ['red', 'black', 'red', 'green', 'black', 'yellow', 'white']
encoder = prep.LabelEncoder().fit(input_labels)
for i, item in enumerate(encoder.classes_):
    print(item, i)
test_labels = ['green', 'red', 'black']
print(encoder.transform(test_labels))
print(list(encoder.inverse_transform([3,0,1])))

values = [-3.3, -1.6, 6.1, 2.4, -1.2, 4.3, -3.2, 5.5, -6.1, -4.4, 1.4, -1.2, 2.1]

threshold = float(values[-1])
vals = np.array(values[:-1], dtype=float)
assert vals.size % 3 == 0
input_data_new = vals.reshape((-1, 3))

binarized = preprocessing.Binarizer(threshold=threshold).transform(input_data_new)
print(binarized)

scaled = preprocessing.scale(input_data_new)
print(scaled.mean(axis=0)); print(scaled.std(axis=0))

minmax = preprocessing.MinMaxScaler((0,1)).fit_transform(input_data_new)
print(minmax)

norm_l1 = preprocessing.normalize(input_data_new, norm='l1')
norm_l2 = preprocessing.normalize(input_data_new, norm='l2')
print(norm_l1); print(norm_l2)

X = np.array([[3.1, 7.2], [4, 6.7], [2.9, 8], [5.1, 4.5],
              [6, 5], [5.6, 5], [3.3, 0.4],
              [3.9, 0.9], [2.8, 1],
              [0.5, 3.4], [1, 4], [0.6, 4.9]])
y = np.array([0,0,0,1,1,1,2,2,2,3,3,3])
clf_lr = linear_model.LogisticRegression(solver='liblinear', C=1)
clf_lr.fit(X, y)
visualize_classifier(clf_lr, X, y)

rng = np.random.default_rng(7)
n_per = 80
means = [(0,0),(3,3),(-3,3)]
covs = [np.array([[0.8,0.2],[0.2,0.5]]),
        np.array([[0.7,-0.1],[-0.1,0.6]]),
        np.array([[0.6,0.0],[0.0,0.6]])]
X_list, y_list = [], []
for i,(m,c) in enumerate(zip(means,covs)):
    Xc = rng.multivariate_normal(m,c, size=n_per)
    X_list.append(Xc); y_list.append(np.full(n_per, i))
X_nb = np.vstack(X_list); y_nb = np.concatenate(y_list)

nb_clf = GaussianNB().fit(X_nb, y_nb)
y_pred = nb_clf.predict(X_nb)
print((y_pred == y_nb).mean())

visualize_classifier(nb_clf, X_nb, y_nb)

Xtr, Xte, ytr, yte = train_test_split(X_nb, y_nb, test_size=0.2, random_state=3, stratify=y_nb)
nb_new = GaussianNB().fit(Xtr, ytr)
yte_pred = nb_new.predict(Xte)
print((yte_pred == yte).mean())

for metric in ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted']:
    scores = cross_val_score(GaussianNB(), X_nb, y_nb, scoring=metric, cv=3)
    print(metric, scores.mean())

rng = np.random.default_rng(123)
n = 10000
actual = rng.binomial(1, 0.45, size=n)
rf_score = 0.2 + 0.6*actual + rng.normal(0, 0.15, size=n)
rf_score = np.clip(1/(1+np.exp(-(rf_score*2-1))), 0, 1)
lr_score = 0.3 + 0.4*actual + rng.normal(0, 0.20, size=n)
lr_score = np.clip(1/(1+np.exp(-(lr_score*2-1))), 0, 1)

df = pd.DataFrame({'actual_label': actual.astype(int),
                   'model_RF': rf_score,
                   'model_LR': lr_score})
df['predicted_RF'] = (df.model_RF >= 0.5).astype(int)
df['predicted_LR'] = (df.model_LR >= 0.5).astype(int)

def startsev_find_TP(y_true, y_pred): return int(((y_true==1)&(y_pred==1)).sum())
def startsev_find_FN(y_true, y_pred): return int(((y_true==1)&(y_pred==0)).sum())
def startsev_find_FP(y_true, y_pred): return int(((y_true==0)&(y_pred==1)).sum())
def startsev_find_TN(y_true, y_pred): return int(((y_true==0)&(y_pred==0)).sum())

def startsev_find_conf_matrix_values(y_true, y_pred):
    TP = startsev_find_TP(y_true,y_pred)
    FN = startsev_find_FN(y_true,y_pred)
    FP = startsev_find_FP(y_true,y_pred)
    TN = startsev_find_TN(y_true,y_pred)
    return TP,FN,FP,TN

def startsev_confusion_matrix(y_true, y_pred):
    TP,FN,FP,TN = startsev_find_conf_matrix_values(y_true,y_pred)
    return np.array([[TN,FP],[FN,TP]])

y_true = df.actual_label.values
y_rf = df.predicted_RF.values
y_lr = df.predicted_LR.values

def startsev_accuracy_score(y_true, y_pred):
    TP,FN,FP,TN = startsev_find_conf_matrix_values(y_true,y_pred)
    return (TP+TN)/(TP+TN+FP+FN)

def startsev_recall_score(y_true, y_pred):
    TP,FN,FP,TN = startsev_find_conf_matrix_values(y_true,y_pred)
    return TP/(TP+FN) if (TP+FN)>0 else 0.0

def startsev_precision_score(y_true, y_pred):
    TP,FN,FP,TN = startsev_find_conf_matrix_values(y_true,y_pred)
    return TP/(TP+FP) if (TP+FP)>0 else 0.0

def startsev_f1_score(y_true, y_pred):
    r = startsev_recall_score(y_true,y_pred)
    p = startsev_precision_score(y_true,y_pred)
    return 2*p*r/(p+r) if (p+r)>0 else 0.0

print(startsev_accuracy_score(y_true, y_rf))
print(startsev_recall_score(y_true, y_rf))
print(startsev_precision_score(y_true, y_rf))
print(startsev_f1_score(y_true, y_rf))

y_rf_025 = (df.model_RF.values >= 0.25).astype(int)
print(startsev_accuracy_score(y_true, y_rf_025))
print(startsev_recall_score(y_true, y_rf_025))
print(startsev_precision_score(y_true, y_rf_025))
print(startsev_f1_score(y_true, y_rf_025))

fpr_RF, tpr_RF, _ = roc_curve(df.actual_label.values, df.model_RF.values)
fpr_LR, tpr_LR, _ = roc_curve(df.actual_label.values, df.model_LR.values)
auc_RF = roc_auc_score(df.actual_label.values, df.model_RF.values)
auc_LR = roc_auc_score(df.actual_label.values, df.model_LR.values)
print(auc_RF); print(auc_LR)

plt.figure()
plt.plot(fpr_RF, tpr_RF, label=f'RF AUC: {auc_RF:.3f}')
plt.plot(fpr_LR, tpr_LR, label=f'LR AUC: {auc_LR:.3f}')
plt.plot([0,1],[0,1], 'k--')
plt.plot([0,0,1,1],[0,1,1,1], 'g--')
plt.legend(); plt.tight_layout(); plt.show()

Xtr, Xte, ytr, yte = train_test_split(X_nb, y_nb, test_size=0.2, random_state=7, stratify=y_nb)
nb_clf = GaussianNB().fit(Xtr, ytr)
svm_clf = SVC(kernel='rbf', C=2.0, gamma='scale', probability=True, random_state=7).fit(Xtr, ytr)

def report(name, y_true, y_pred):
    print(name,
          accuracy_score(y_true,y_pred),
          precision_score(y_true,y_pred,average='weighted'),
          recall_score(y_true,y_pred,average='weighted'),
          f1_score(y_true,y_pred,average='weighted'))

report('Naive Bayes', yte, nb_clf.predict(Xte))
report('SVM (RBF)', yte, svm_clf.predict(Xte))

visualize_classifier(nb_clf, X_nb, y_nb)
visualize_classifier(svm_clf, X_nb, y_nb)